{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##The engine of neural networks:Gradient-based optimization\n",
    "`output = relu(dot(input, W) + b)`\n",
    "W and b are tensors that are attributes of the layer.They're called the weight or trainable parameters of the layer.(kernel and bias)\n",
    "First,these weight matrices are filled with small random values(random initialization).\n",
    "Next,gradually adjust these weights,based on a feedback signal.(training)\n",
    "###training loop\n",
    "Repeat these steps in a loop, until the loss seems sufficiently low:\n",
    "+ Draw a batch of training samples, x, and corresponding targets, y_true.\n",
    "+ Run the model on x (a step called the forward pass) to obtain predictions, y_pred.\n",
    "+ Compute the loss of the model on the batch, a measure of the mismatch between y_pred and y_true.\n",
    "+ Update all weights of the model in a way that slightly reduces the loss on this batch.\n",
    "\n",
    "One naive solution that freeze all weights in the model except the one scalar coefficient being considered is inefficient.We are needed to compute two forward passes.\n",
    "### Much better approach:gradient descent\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
