{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##The engine of neural networks:Gradient-based optimization\n",
    "`output = relu(dot(input, W) + b)`\n",
    "W and b are tensors that are attributes of the layer.They're called the weight or trainable parameters of the layer.(kernel and bias)\n",
    "First,these weight matrices are filled with small random values(random initialization).\n",
    "Next,gradually adjust these weights,based on a feedback signal.(training)\n",
    "###training loop\n",
    "Repeat these steps in a loop, until the loss seems sufficiently low:\n",
    "+ Draw a batch of training samples, x, and corresponding targets, y_true.\n",
    "+ Run the model on x (a step called the forward pass) to obtain predictions, y_pred.\n",
    "+ Compute the loss of the model on the batch, a measure of the mismatch between y_pred and y_true.\n",
    "+ Update all weights of the model in a way that slightly reduces the loss on this batch.\n",
    "\n",
    "One naive solution that freeze all weights in the model except the one scalar coefficient being considered is inefficient.We are needed to compute two forward passes.\n",
    "### Much better approach:gradient descent\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "past_velocity = 0\n",
    "momentum = 0.1 #不变的动量因子\n",
    "# while loss>0.01:\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "x = tf.Variable(0.)  # 将标量Variable的值初始化为0\n",
    "with  tf.GradientTape() as tape: #创建一个GradientTape作用域\n",
    "    y = 2 * x +3  #在作用域内，对变量进行张量运算\n",
    "    grad_of_y_wrt_x = tape.gradient(y,x) #利用梯度带获取输出y相对于变量x的梯度"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# GradientTape 也可以用于张量运算\n",
    "x = tf.Variable(tf.zeros(2,2)) # 将Variable初始化为形状（2，2）的零张量\n",
    "with tf.GradientTape() as tape:\n",
    "    y = 2 * x + 3\n",
    "grad_of_y_wrt_x = tape.gradient(y,x) #grad_of_y_wrt_x是一个形状为(2,2)的张量，与x相同\n",
    "#表示y = 2 * x + 3 在x = [[0,0],[0,0]]附近的曲率\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
      "array([[0.68255615, 0.68255615],\n",
      "       [1.2405155 , 1.2405155 ]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 2.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# 适用于变量列表\n",
    "W = tf.Variable(tf.random.uniform((2,2)))\n",
    "b = tf.Variable(tf.zeros((2,)))\n",
    "x = tf.random.uniform((2,2))\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.matmul(x,W)+b # 在TensorFlow中matmul是指点积\n",
    "grad_of_y_wrt_W_and_b = tape.gradient(y,[W,b]) #grad_of_y_wrt_W_and_b是由两个张量组成的列表\n",
    "#这两个张量的形状分别于W和b相同\n",
    "print(grad_of_y_wrt_W_and_b)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
